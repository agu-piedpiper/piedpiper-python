{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1LxOD41OmFxNkTchFykIkagx90DxAGR5q",
      "authorship_tag": "ABX9TyMcRMceSgBaxMXuB+EBN5L1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARAN1218/piedpiper-python/blob/main/PPP%E2%91%A4_Sazae_janken_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# サザエさんの手を予測して、じゃんけんで圧勝しよう！\n",
        "Python講座の終着点、人工知能編へようこそ！  \n",
        "Pythonはできることが様々あるのですが、やはり特筆すべきはデータ分析・機械学習のしやすさでしょう！  \n",
        "ここではPythonの使い道の中で最も面白い機械学習をジャンケンAIを作ることで体験し、その沼へ進んでいきましょう！"
      ],
      "metadata": {
        "id": "j-khm9bhQ-zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "皆さんはサザエさんを知っていますか？サザエさんは西暦１９６９年（昭和４４年）１０月５日（日曜日）から今までずっと放送されている国民的アニメーションですが、西暦１９９１年（平成３年）の秋に突然じゃんけんが行われたことはあまり知られていないはずです。小さい頃皆さんはこのじゃんけんに気軽に挑戦していたことと思われますが、これに対して初回からデータを取り続け、それを分析することで驚異の勝率8割越えを達成しているヤバい団体があります。つまり、我々も努力次第でサザエさんに圧勝できる可能性があるのです！！！  \n",
        "今回は強敵・サザエさんに対し、じゃんけんAIを作ることで挑戦していきましょう！"
      ],
      "metadata": {
        "id": "SVsSn6goF_h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "とりあえずこのcolabをコピーして、マイドライブの「PiedPiper_Python_個人用」というフォルダの中に入れる作業をしましょう。そうしてから本編スタートです。"
      ],
      "metadata": {
        "id": "9c_1Uw0_JRo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 各種インポート"
      ],
      "metadata": {
        "id": "Y4E1_Ek_QdEy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5e_IwyIQYal"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from time import sleep\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ収集\n",
        "敵を知り、己を知れば百戦危うからず...  \n",
        "まずは敵を知ることが大切です。敵を知るには敵のデータが必要不可欠！  \n",
        "**偶然にも**敵の全対戦データを保有している[サイト](http://park11.wakwak.com/~hkn/bunseki0.htm)があるらしい。しかし、その情報を一つ一つ手動でエクセルとかに入力するのは骨が折れる...  \n",
        "そこで！そんな退屈なことはPythonにやってもらいましょ〜"
      ],
      "metadata": {
        "id": "hD0t3yoOQ6-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## じゃんけんデータ（all）"
      ],
      "metadata": {
        "id": "VyElE5gfowkb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# http://park11.wakwak.com/~hkn/data{}.htm\n",
        "\n",
        "# データを取得したいサイトのURLを文字列として保管する\n",
        "# ここで、「/data{}.htm」と{}が入っている理由は、後でfstringのテクニックを使ってURLを変更したいからです！\n",
        "url = \"http://park11.wakwak.com/~hkn/data{}.htm\"\n",
        "\n",
        "# とりあえず1991年度のデータを取得したいので、fstringで目的のページにアクセスできるurlに書き換える\n",
        "target_url = url.format(1991)\n",
        "\n",
        "# target_urlのページにアクセスし、ページの文字情報等を変数rに入れる\n",
        "r = requests.get(target_url)\n",
        "\n",
        "# 連続でアクセスすると犯罪になる場合があるので、requests一回につき一秒以上待機時間をとる\n",
        "# (今回は一回しかリクエストしないから本当はいらないけど)\n",
        "sleep(1)\n",
        "\n",
        "# rに入っているhtmlの文字情報を解析し、変数soupに入れる\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "# 取得したページ全体を表示してみましょう！\n",
        "print(soup)"
      ],
      "metadata": {
        "id": "Spib1be-Q6f8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ヨシ！とりあえずサイトの情報を取得できましたね！  \n",
        "ただ、このままだとやはり分析には使えませんよね...どうします？コピペでエクセルとかに入力しますか？  \n",
        "\n",
        "違いますよね！退屈なことはPythonにやらせましょう！！"
      ],
      "metadata": {
        "id": "KkV7DDswT88q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = soup.find('pre').text\n",
        "data"
      ],
      "metadata": {
        "id": "DLfoLcPJTqlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "テキスト中に「\\n」が入ってて、汚いですよね。  \n",
        "そんな時はそれを取り去って、かつそれを利用してデータを綺麗にリストに入れてやりましょう！"
      ],
      "metadata": {
        "id": "Ujn_sUi7VDL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.strip(\"\\n\")\n",
        "data = data.split('\\n')\n",
        "data"
      ],
      "metadata": {
        "id": "zteEHBsdVBRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [d.split(' ') for d in data]\n",
        "data"
      ],
      "metadata": {
        "id": "qdZynWWVVoMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas.DataFrame型のデータに変換する！\n",
        "# データ分析には必須級の便利なやつです！\n",
        "data = pd.DataFrame(data)\n",
        "data"
      ],
      "metadata": {
        "id": "KdxacbjOVV2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1992以降もまとめてやりましょー\n",
        "\n",
        "url = \"http://park11.wakwak.com/~hkn/data{}.htm\"\n",
        "data_list = []\n",
        "\n",
        "for year in tqdm(range(1991, 2023+1)): # tqdmを使うと、for文の進捗を可視化できる！(後かっこいい)\n",
        "  target_url = url.format(year) # year年のデータにアクセスできるURLに変換する！\n",
        "  r = requests.get(target_url)\n",
        "  sleep(1)\n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  data = soup.find('pre').text\n",
        "  data = data.strip(\"\\n\")\n",
        "  data = data.split('\\n')\n",
        "  data = [d.split(' ') for d in data]\n",
        "  data_list.append(data) # 逐一データフレームに変換するのではなく、一旦リストに入れておく\n",
        "\n",
        "df_all = pd.DataFrame(data_list) # 最後に一気に変換させる！\n",
        "df_all.head() # データの先頭5行のみ表示させる"
      ],
      "metadata": {
        "id": "QHZVR70eV6lI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 試しに最後に取得した2023年度のデータを見てみると...？\n",
        "data"
      ],
      "metadata": {
        "id": "szzCcwXRYKFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "作戦を変更する"
      ],
      "metadata": {
        "id": "TuHQrze4aJg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"http://park11.wakwak.com/~hkn/data{}.htm\"\n",
        "data_list = pd.DataFrame() # 保管用のデータフレーム\n",
        "\n",
        "for year in tqdm(range(1991, 2023+1)): # tqdmを使うと、for文の進捗を可視化できる！(後かっこいい)\n",
        "  target_url = url.format(year) # year年のデータにアクセスできるURLに変換する！\n",
        "  r = requests.get(target_url)\n",
        "  sleep(1)\n",
        "  soup = BeautifulSoup(r.content, 'html.parser')\n",
        "  data = soup.find('pre').text\n",
        "  data = data.strip(\"\\n\")\n",
        "  data = data.split('\\n')\n",
        "  data = [d.split(' ') for d in data]\n",
        "  data = pd.DataFrame(data) # 今度は逐一データフレームに変換して、それを保管用のデータフレームに連結させる作戦でいく！\n",
        "  data_list = pd.concat([data_list, data], axis=0)\n",
        "\n",
        "df_all = pd.DataFrame(data_list)\n",
        "df_all"
      ],
      "metadata": {
        "id": "Xdq9kebKYalx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NaNってNaNNaNだ？？  \n",
        "NaNとは、存在しないデータに対してとりあえず割り当てられるハズレの値のことNaNです。今回のデータで言うと、第一回の行には存在しなかった列でも後ろの方の行には存在した列があり、その列のデータが無理やり作られてしまったことによりNaNが生じてしまったのですね。  \n",
        "とりあえずこれをNaNとかしよう！...と考えるのはNaNセンスで、重要な情報が取得できているかもしれないので確認するのが無NaNでしょう。"
      ],
      "metadata": {
        "id": "tyr5Gt7gaW0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4列目と5列目に入っているデータの正体を見る\n",
        "print(df_all[3].unique())\n",
        "print()\n",
        "print(df_all[4].unique())"
      ],
      "metadata": {
        "id": "Zv0gM6c5Yj5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "列名(カラム名)が番号でダサい＆分かりにくいので、カラム名を変更しよう！  \n",
        "この時、**カラム名を見ただけで列のデータの種類やデータ型が把握できる**とgoodですね。  \n",
        "今回は以下のように命名しましょー\n",
        "\n",
        "| 変更前 | 変更後 | 列の意味 |\n",
        "| :-: | :-: | :-: |\n",
        "| 0 | seq | 放送回数 |\n",
        "| 1 | date | 日付 |\n",
        "| 2 | hand | 宿敵・サザエさんが出したジャンケンの手 |\n",
        "| 3 | detail1 | 補足情報① |\n",
        "| 4 | detail2 | 補足情報② |"
      ],
      "metadata": {
        "id": "q_anM-O-awS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 列名を変更する\n",
        "df_all = df_all.set_axis(['seq', 'date', 'hand', 'detail1', 'detail2'], axis=1) # axisは行：０、列：1で覚えましょう。もしくはUSBメモリみたいに2分の１を引きましょう。\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "ZQI-mxnsXBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 収集したデータを保存してみよう！\n",
        "# df_all.to_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_raw.csv\", index=False)"
      ],
      "metadata": {
        "id": "k5Mr9lHWFZsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 保存したデータを読み込む！\n",
        "df_all = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_raw.csv\")\n",
        "df_all"
      ],
      "metadata": {
        "id": "aNHKSibjGr-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "どうです？簡単にデータをゲットできたでしょ？  \n",
        "ちなみにこの技術を「**スクレイピング**」と言います。英単語のscrape(こする)から来ています。"
      ],
      "metadata": {
        "id": "cmYSBkDCSFoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## じゃんけんデータ（describe）\n",
        "サザエさんじゃんけん研究所のサイトには各年度の集計情報があるので、それも取得してしまいましょう！  \n",
        "今度はさっきより断然簡単ですよ〜"
      ],
      "metadata": {
        "id": "OyPotW9Oo2_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pandasのread_htmlメソッドを使うことで、tableタグのデータをたった一行でスクレイピングできます！\n",
        "# リストとして取得するので、その一つ目の要素を取り出す意味で[0]をつける\n",
        "df_describe = pd.read_html('http://park11.wakwak.com/~hkn/bunseki0.htm')[0]\n",
        "df_describe"
      ],
      "metadata": {
        "id": "pM2_tFUahnJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これも保存しておく\n",
        "#df_describe.to_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_describe_raw.csv\", index=False)"
      ],
      "metadata": {
        "id": "m3i9QnXE5LH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 再度呼び出し\n",
        "df_describe = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_describe_raw.csv\")\n",
        "df_describe"
      ],
      "metadata": {
        "id": "8bYSDHn25S5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ前処理\n",
        "データが汚ったねぇから綺麗にする"
      ],
      "metadata": {
        "id": "6-cpfYnvgYiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df_describe"
      ],
      "metadata": {
        "id": "GSPV4Rmb_5CF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# まずはdf_describeから\n",
        "columns = df_describe.iloc[0,:] # 一行目を列名にする為に取得する\n",
        "columns[0] = \"年\" # NaNと入れ替え\n",
        "df_describe = df_describe.set_axis(columns, axis=1)\n",
        "df_describe = df_describe.iloc[1:,:] # 不要な行を取り除く\n",
        "df_describe = df_describe.reset_index(drop=True)\n",
        "df_describe"
      ],
      "metadata": {
        "id": "rUj5JZgFh40V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# それぞれの手を回数と割合に分割する\n",
        "# やってる操作：\n",
        "#   1. .str.replace(\")\", \"\")...「)」の文字を消す\n",
        "#   2. .str.split(\"(\")...「(」の文字を軸にテキストを二分割する ex.) 18(34.6% → 18 & 34.6%\n",
        "#   3.  .apply(pd.Series)...列を二つに分割する\n",
        "#   4.  .set_axis(['〇〇(回数)', '〇〇(割合)'], axis=1)...列名を変更する\n",
        "# こんな感じで一行にまとめて処理かけるけど、見ての通り可読性が落ちるから、チームで分析するときはやらないようにしよう！\n",
        "gu = df_describe['グー'].str.replace(\")\", \"\").str.split(\"(\").apply(pd.Series).set_axis(['グー(回数)', 'グー(割合)'], axis=1)\n",
        "choki = df_describe['チョキ'].str.replace(\")\", \"\").str.split(\"(\").apply(pd.Series).set_axis(['チョキ(回数)', 'チョキ(割合)'], axis=1)\n",
        "pa = df_describe['パー'].str.replace(\")\", \"\").str.split(\"(\").apply(pd.Series).set_axis(['パー(回数)', 'パー(割合)'], axis=1)\n",
        "\n",
        "# それらを横方向に結合する\n",
        "df_describe = pd.concat([df_describe, gu, choki, pa], axis=1)\n",
        "\n",
        "# 分割前の不要な列を消す\n",
        "df_describe = df_describe.drop(['グー', 'チョキ', 'パー'], axis=1)\n",
        "df_describe"
      ],
      "metadata": {
        "id": "dyGGcDcH2G7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 不要な文字（「年」「%」）を一気に取り除く\n",
        "df_describe['年'] = df_describe['年'].str.replace('年', '')\n",
        "df_describe['グー(割合)'] = df_describe['グー(割合)'].str.replace('%', '')\n",
        "df_describe['チョキ(割合)'] = df_describe['チョキ(割合)'].str.replace('%', '')\n",
        "df_describe['パー(割合)'] = df_describe['パー(割合)'].str.replace('%', '')\n",
        "df_describe\n",
        "\n",
        "# 実は以下のようにまとめて処理もできたりする\n",
        "# df_describe.applymap(lambda x : x.translate(str.maketrans({'年':'', '%':''})))"
      ],
      "metadata": {
        "id": "LJ0jpWq12bwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後にデータ型を数値に変える\n",
        "# 変換前のデータ型\n",
        "print(df_describe.dtypes)\n",
        "print()\n",
        "\n",
        "# astypeメソッドは辞書でデータ型を指定できる\n",
        "astype_dict = {\n",
        "    '年':str,\n",
        "    '合計':int,\n",
        "    'グー(回数)':int,\n",
        "    'グー(割合)':float,\n",
        "    'チョキ(回数)':int,\n",
        "    'チョキ(割合)':float,\n",
        "    'パー(回数)':int,\n",
        "    'パー(割合)':float\n",
        "}\n",
        "df_describe = df_describe.astype(astype_dict)\n",
        "\n",
        "# 変換後のデータ型\n",
        "print(df_describe.dtypes)"
      ],
      "metadata": {
        "id": "B60HQZZM89zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 完成したデータを保存する\n",
        "#df_describe.to_csv('drive/MyDrive/PiedPiper_Python_個人用/sazae_describe.csv', index=False)"
      ],
      "metadata": {
        "id": "NPDs3G_L9JE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## df_all"
      ],
      "metadata": {
        "id": "jp_nX27f_15n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 次にdf_allをやる\n",
        "# データが多いので、value_countsメソッドで各種値を確認\n",
        "for column in df_all.columns:\n",
        "  display(df_all[column].value_counts())\n",
        "\n",
        "# handカラムにじゃんけんの手以外のデータが入っている→不要なデータの条件に使えそう"
      ],
      "metadata": {
        "id": "Rs3Z5HvfE41h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NaNが入っているかどうかを調べるには以下\n",
        "for column in df_all.columns:\n",
        "  print(column)\n",
        "  print(sum(df_all[column].isna())) # isnaメソッドでTrue・Falseの一列にし、その内のTrue(=NaNの行)を合計することでNaNの個数が分かる\n",
        "  print()"
      ],
      "metadata": {
        "id": "2-iGQs8imB0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handカラムが「グー」「チョキ」「パー」以外であれば、その列を消す\n",
        "df_all = df_all[df_all[\"hand\"].isin([\"グー\", \"チョキ\", \"パー\"])]\n",
        "df_all = df_all.reset_index(drop=True) # indexが飛び飛びになってしまっているので、リセットする\n",
        "df_all"
      ],
      "metadata": {
        "id": "qSHbBhsUHgoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の〇〇rowsのところを見てください。seqデータの最後の回数と同じになってますか？  \n",
        "同じになってたら成功です！"
      ],
      "metadata": {
        "id": "gW7Xg0MDIgR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次はdateのところをもっと便利に使える形に変換しましょう。"
      ],
      "metadata": {
        "id": "QkDd-wb9SIpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.to_datetime(df_all['date'], format='%Y年%m月%d日')"
      ],
      "metadata": {
        "id": "Jzw3T1r2TRwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "エラー出たので確認しましょう。\n",
        "どうやら「(1回目)」という文字列が紛れ込んでいるようですね..."
      ],
      "metadata": {
        "id": "ye9T-BecTXkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# queryメソッドを使うことで、条件を満たす行を簡単にゲットできる。\n",
        "# ただし、以下のstr.containsメソッドを使うときに限り、engine='python'とやる必要がある。(その他の操作はいらない)\n",
        "df_all.query('date.str.contains(\"回\")', engine='python')"
      ],
      "metadata": {
        "id": "ngsPSp57Sze6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ピンポイントで修正→(1回目)と(2回目)を削除\n",
        "df_all['date'] = df_all['date'].map(lambda x : x.split(\"日\")[0])\n",
        "df_all.query('date.str.contains(\"回\")', engine='python')\n",
        "\n",
        "# ちなみに、もしこのデータを削除するなら以下です\n",
        "# df_all = df_all[~df_all[\"seq\"].isin([\"第1113回\", \"第1114回\"])] # 条件式の先頭に「~」を入れれば、反対の条件を満たす行を探してくれる。\n",
        "# df_all = df_all.reset_index(drop=True) # indexが飛び飛びになってしまっているので、リセットする\n",
        "# df_all"
      ],
      "metadata": {
        "id": "Kv81I7cTT2Lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 扱いやすくするため、dateを時間型データに直します\n",
        "df_all['date'] = pd.to_datetime(df_all['date'], format='%Y年%m月%d')\n",
        "df_all.dtypes"
      ],
      "metadata": {
        "id": "jRVrPeaJSN7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 後で分析しやすくするため、dateを年・月・日に分解します\n",
        "dates = pd.concat([df_all['date'].dt.year, df_all['date'].dt.month, df_all['date'].dt.day], axis=1).set_axis(['year' , 'month', 'day'], axis=1)\n",
        "df_all = pd.concat([df_all, dates], axis=1)\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "FL6Uf9iUAXvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# これも保存しておきましょう\n",
        "#df_all.to_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken.csv\", index=False)"
      ],
      "metadata": {
        "id": "NFYc0SnjO31M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken.csv\")\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "dN_FqqQJAX93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  特徴量エンジニアリング\n",
        "予測に用いるデータを作りましょう！  \n",
        "今回一緒に作っていく特徴量は以下の3つです！\n",
        "\n",
        "1. 1回前のじゃんけんの手\n",
        "2. 2回前のじゃんけんの手\n",
        "3. 新年一回目の放送フラグ\n",
        "\n",
        "何故この３つなのかというと、先行研究で予測への有効性が示唆されているからです。皆さんもデータ分析する際は**絶対**に先行研究を調べましょう。役に立ちますよ！"
      ],
      "metadata": {
        "id": "rAnnU53WqNHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを呼び戻す\n",
        "df_all = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken.csv\")\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "pUMnFPf99WTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## １・２回前のじゃんけんの手のデータを持つ列"
      ],
      "metadata": {
        "id": "uq9hFe7vQZCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all[\"hand_pre1\"] = df_all[\"hand\"].shift(1, fill_value=\"パー\")\n",
        "df_all[\"hand_pre2\"] = df_all[\"hand\"].shift(2, fill_value=\"パー\")\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "uO8xrIO3Ifxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 計算に使う用に一行ずつ変換しておく\n",
        "# これをダミー変数化と言います\n",
        "a = pd.get_dummies(df_all['hand_pre1'], drop_first=True).set_axis(['チョキ_pre1', 'パー_pre1'], axis=1)\n",
        "b = pd.get_dummies(df_all['hand_pre2'], drop_first=True).set_axis(['チョキ_pre2', 'パー_pre2'], axis=1)\n",
        "\n",
        "df_all = pd.concat([df_all, a, b], axis=1)\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "sxwD9EMM8719"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes!　無事に作れましたね！"
      ],
      "metadata": {
        "id": "2mjrZPEvRMFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 新年一回目の放送フラグ"
      ],
      "metadata": {
        "id": "Tm6oyJuuQztL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# その行が新年一発目かどうかを判断し、そうであれば1、そうでなければ0を保管する列を作れば良い\n",
        "df_all['is_first'] = 0\n",
        "date_list = []\n",
        "date_index = 0\n",
        "\n",
        "def is_first(row):\n",
        "  global date_list\n",
        "  global date_index\n",
        "  if row['year'] not in date_list:\n",
        "    #row['is_first'] = 1\n",
        "    df_all['is_first'][date_index] = 1\n",
        "    date_list.append(row['year'])\n",
        "  date_index += 1\n",
        "\n",
        "df_all.apply(lambda x : is_first(x), axis=1)\n",
        "df_all['is_first'].value_counts()"
      ],
      "metadata": {
        "id": "242uLMG3RKRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データ分析(EDA)\n",
        "\n",
        "次に、作った特徴量とサザエさんの出す手に関連性がないか確認します。まあ本当はデータ分析のターンは段階的に行われるものではなくて、前処理→データ分析→(その結果を基に)特徴量エンジニアリングのサイクルを細かく回していき、より良いデータを作っていきます。  \n",
        "尚、これらの分析をカッコつけて探索的データ分析（Explanatory Data Analysis）と言ったりしますw"
      ],
      "metadata": {
        "id": "UN3ujj-mdNLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "とりあえず、今回は先ほど作った3つのデータの検証をしてみましょう。"
      ],
      "metadata": {
        "id": "VhMZBdKqUUmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# クロス集計したい時はpd.crosstab()を使う\n",
        "pd.crosstab(df_all['year'], df_all['hand_pre1'])\n",
        "pd.crosstab(df_all['year'], df_all['hand_pre2'])"
      ],
      "metadata": {
        "id": "vOVFy1uSeVK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1回前のじゃんけんの手\n",
        "pd.crosstab(df_all['hand'], df_all['hand_pre1'])"
      ],
      "metadata": {
        "id": "r-MVB-o8TtOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のクロス集計表を見ると、前の手と同じ手を次回に出しづらいことが分かりますよね！  \n",
        "2回前のじゃんけんの手ではどうなるでしょうか？？"
      ],
      "metadata": {
        "id": "F_LagR-5i4s7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2回前のじゃんけんの手\n",
        "pd.crosstab(df_all['hand'], df_all['hand_pre2'])"
      ],
      "metadata": {
        "id": "AxFWTcoAUuL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "やはりこちらも出しづらいことが分かります！これらは特徴量として加えるべきです！！  \n",
        "次は新年一回目の放送での出す手を見てみましょう"
      ],
      "metadata": {
        "id": "kvxtHqlVjOK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 新年一回目の放送フラグ\n",
        "pd.crosstab(df_all['hand'], df_all['is_first'])"
      ],
      "metadata": {
        "id": "0veC4Xu3UuOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "事前調査通り、年初の放送のジャンケンでは圧倒的にチョキを出しやすいことがデータから分かります。  \n",
        "これは特徴量に組み込まない手はないですね...！"
      ],
      "metadata": {
        "id": "l74nP_VmfIsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "さて、今まで放置してたdetailに触れていきましょう。detail2に関してはそもそもNaNでないデータが2つしかないため、削除の方向でいいでしょう。しかしdetail1はどうでしょうか？調べてみましょう！"
      ],
      "metadata": {
        "id": "PddgcZ_jhBst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['is_event'] = df_all['detail1'].notna()\n",
        "df_all['is_event'] = df_all['is_event'].astype(int)\n",
        "pd.crosstab(df_all['hand'], df_all['is_event'])"
      ],
      "metadata": {
        "id": "RyOzj-PYf65N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "仮説通り、イベントがある時はチョキが出やすそうです！  \n",
        "これも特徴量に入れましょう"
      ],
      "metadata": {
        "id": "-gzzdg6Qg345"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ジャンケンの手を計算的に扱いたいので、handカラムのグーチョキパーを0,1,2に変換する\n",
        "df_all['target'] = df_all['hand'].map(lambda x : 0 if x=='グー' else 1 if x=='チョキ' else 2)\n",
        "df_all['target'].value_counts()"
      ],
      "metadata": {
        "id": "S_s5UpEn-s89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# このデータを最終データとして保存しておく\n",
        "#df_all.to_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_final.csv\", index=False)"
      ],
      "metadata": {
        "id": "0qKqLj5cUwsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 予測モデル作成\n",
        "お待たせしました。一番楽しいところです。  \n",
        "早速AI作り...ではなく、自作AIの実力を評価するための関数を作りましょう。でなければ、本当に強いジャンケン予測器が作れたのかどうか分かりませんからね。"
      ],
      "metadata": {
        "id": "CED34O9BmAhz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "流れとしては、ランダムにジャンケンするモデルを作り、それで評価関数をテストする感じです。"
      ],
      "metadata": {
        "id": "ERzgsFJ8kLvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを呼び戻す\n",
        "df_all = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_final.csv\")"
      ],
      "metadata": {
        "id": "bwmcBmlzVCjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最もテキトーなモデル...ランダムモデル！\n",
        "class random_model:\n",
        "  def __init__(self):\n",
        "    self.hands = {0:\"グー\", 1:\"チョキ\", 2:\"パー\"}\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    return \n",
        "\n",
        "  def predict(self, x):\n",
        "    return pd.Series([random.randint(0,2) for i in range(len(x))], name='pred')\n"
      ],
      "metadata": {
        "id": "vteN9KXXmE06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 試しに予測してみる\n",
        "# (ランダムモデルでは使わないけど)学習データとテストデータを作る\n",
        "train = df_all.query(\"year < 2022\")\n",
        "x_train = train[['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']]\n",
        "y_train = train['target']\n",
        "\n",
        "test = df_all.query(\"year >= 2022\").reset_index(drop=True)\n",
        "x_test = test[['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']]\n",
        "y_test = test['target'].rename('true')\n",
        "\n",
        "# 予測フェーズ\n",
        "r_model = random_model()\n",
        "r_model.fit(x_train, y_test)\n",
        "pred = r_model.predict(x_test)\n",
        "pd.concat([pred, y_test], axis=1)"
      ],
      "metadata": {
        "id": "tbHsxcS_mk6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "「モデルを評価する」とは、つまり「モデルがどれだけ正確に予測できるかを定量的に表す」ことです。今回はついでなので分類タスクにおける種々の評価指標について実装していきましょう。"
      ],
      "metadata": {
        "id": "37h6wKThkkiI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回評価に使う指標は、「正解率」「勝率」です。  \n",
        "「**正解率**」は私たちが普段使う意味の正解率の考え方に近いです。要は全予測中で正解ラベルと同じだった割合のことですね。\n",
        "\n",
        "また、今回はジャンケンをするのですから、「**勝率**」も重要かつ気になるでしょう。勝率の定義はサザエさんジャンケン研究所と同じ定義で実装します。つまり、「**引き分けを分母から除いたジャンケンで勝った割合**」ですね。"
      ],
      "metadata": {
        "id": "gs3Vb1uak3BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "その他にも分類タスクには色々な種類の評価指標があります。 \n",
        "参考：https://zero2one.jp/ai-word/accuracy-precision-recall-f-measure/"
      ],
      "metadata": {
        "id": "HWEgKY1hlECO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.window.doc import window_agg_numba_parameters\n",
        "# 何回かの過去データを使ってモデルを評価する関数を作る\n",
        "# 実装対象：「正解率」「適合率」「再現率」「F値」「勝率」\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def janken_eval(model, data, columns):\n",
        "  # print('classification_report = \\n', classification_report(y_true=Y_test, y_pred=Y_pred))\n",
        "  # print('confusion matrix = \\n', confusion_matrix(y_true=Y_test, y_pred=Y_pred))\n",
        "  # print('accuracy = ', accuracy_score(y_true=Y_test, y_pred=Y_pred))\n",
        "  # print('precision = ', precision_score(y_true=Y_test, y_pred=Y_pred), average='micro')\n",
        "  # print('recall = ', recall_score(y_true=Y_test, y_pred=Y_pred))\n",
        "  # print('f1 score = ', f1_score(y_true=Y_test, y_pred=Y_pred))\n",
        "\n",
        "  # 勝率\n",
        "  total_win, total_draw, total_lose, total_accuracy = 0, 0, 0, 0\n",
        "  player_hand_trans = {0:2, 1:0, 2:1}\n",
        "  d_list = []\n",
        "\n",
        "  for year in range(1995,2023+1):\n",
        "    train = data.query(f\"year < {year}\").reset_index(drop=True)\n",
        "    test = data.query(f\"year == {year}\").reset_index(drop=True)\n",
        "    train_x, train_y = train[columns], train['target']\n",
        "    test_x, test_y = test[columns], test['target']\n",
        "\n",
        "    # parameters = [params]\n",
        "    # clf_cv = GridSearchCV(model(), parameters, n_jobs = -1)\n",
        "    # clf_cv.fit(train_x, train_y)\n",
        "    # clf = model(C=clf_cv.best_estimator_.C, random_state=71)\n",
        "    model.fit(train_x, train_y)\n",
        "    pred = model.predict(test_x)\n",
        "\n",
        "    df_y = pd.DataFrame([pred, test_y], index=['pred', 'true']).T\n",
        "    accuracy = accuracy_score(y_true=df_y['true'], y_pred=df_y['pred'])\n",
        "    total = len(df_y)\n",
        "    win = len(df_y[df_y['pred'] == df_y['true']])\n",
        "    df_y['pred'] = df_y['pred'].map(lambda x : player_hand_trans[x])\n",
        "    draw = len(df_y[df_y['pred'] == df_y['true']])\n",
        "    lose = total - win - draw\n",
        "\n",
        "    d = {}\n",
        "    d[\"year\"] = year\n",
        "    d[\"win\"] = win\n",
        "    d[\"draw\"] = draw\n",
        "    d[\"lose\"] = lose\n",
        "    d[\"winning_percentage\"] = f'{(d[\"win\"] / (d[\"win\"]+d[\"lose\"]))*100:.3f}'\n",
        "    d[\"accuracy\"] = accuracy\n",
        "    d_list.append(d)\n",
        "\n",
        "  print()\n",
        "  df_final = pd.DataFrame(d_list)\n",
        "  total_win, total_draw, total_lose, total_accuracy =  df_final.sum()[[1,2,3,5]]\n",
        "  display(pd.DataFrame(d_list))\n",
        "\n",
        "  print(f'TOTAL：win:  {total_win}, draw:  {total_draw}, lose:  {total_lose}, winning percentage = {(total_win / (total_win+total_lose))*100:.3f}, accuracy = {total_accuracy/(2023+1-1995):.3f}')\n"
      ],
      "metadata": {
        "id": "gsge7OjknUHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ランダムモデルで実験\n",
        "r_model = random_model()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "janken_eval(r_model, df_all, columns)"
      ],
      "metadata": {
        "id": "2hTRQoLQA-nD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでよく頑張りましたね！いよいよ機械学習モデルを作りますよ！  \n",
        "\n",
        "今回試す機械学習アルゴリズムはロジスティック回帰・サポートベクターマシン(SVM)・ランダムフォレストという手法です。それぞれ良い所はあるのですが、予測能力に限って言えば\n",
        "\n",
        "ロジスティック回帰 ＜ SVM ＜ ランダムフォレスト\n",
        "\n",
        "の順に強いです。（＝複雑な予測対象でも対応できる）  \n",
        "さて、予測モデルの強さによってジャンケンの勝率が変わるのでしょうか！？"
      ],
      "metadata": {
        "id": "VRB4mct0_cNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "lr_model = LogisticRegression()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "janken_eval(lr_model, df_all, columns)"
      ],
      "metadata": {
        "id": "u6RyIIZQ_bd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm_model = SVC(C=1)\n",
        "params = {'C': np.logspace(-2, -1, 10)}\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "janken_eval(svm_model, df_all, columns)"
      ],
      "metadata": {
        "id": "R9nR3wbNFDrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "janken_eval(rf_model, df_all, columns)"
      ],
      "metadata": {
        "id": "ne6yeM0MFDuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 最後に使うので、最も性能が良かったモデルを保存する\n",
        "# モデル保存\n",
        "#filename = 'drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_model.pickle'\n",
        "#pickle.dump(lr_model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "TlqZZY8jrNY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "いかがでしたか？データ分析の力を使えば、サザエさん程度圧倒するのは造作もないのです（？）  \n",
        "今回はあくまでサザエさんとのジャンケンにおける「お遊びモデル」として実装しました。  \n",
        "ただ、このモデルを存分に活かす機会ならありますよね？来週のサザエさんとのジャンケンはこのモデルで戦いましょう（？）"
      ],
      "metadata": {
        "id": "91yKjEs3K0xe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ分析の章に戻って新しい特徴量を見つければ、もっと勝率を上げられるかもしれません。  \n",
        "ぜひ試してみてください！！👋  \n",
        "\n",
        "アイデア：\n",
        "1. その手が出ていない期間（前回出た＝１、前々回出た＝２...）\n",
        "2. ある年におけるその手が出た割合"
      ],
      "metadata": {
        "id": "y7p9u4-GcCou"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 予測アプリ制作\n",
        "せっかくサザエさんをボコれるAIを作ったのですから、その実力を試さなくては勿体無いですよねぇ？！  \n",
        "最後にこのAIを使うインターフェースを整え、アプリケーションにしてしまいましょう！  \n",
        "そして今週のサザエさんの手を予測し、勝利しましょう！"
      ],
      "metadata": {
        "id": "k5jDUc67qOIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測モデル・データを呼び出す\n",
        "filename = 'drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_model.pickle'\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "df_all = pd.read_csv(\"drive/MyDrive/PiedPiper_Python_個人用/sazae_janken_final.csv\")\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "janken_eval(loaded_model, df_all, columns)"
      ],
      "metadata": {
        "id": "MA0odb3GqmtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測アプリ（関数）を作る\n",
        "def sazae_janken(model, columns):\n",
        "  data_for_pred = {} # 辞書型の空データを作る\n",
        "  trans_hands = {0:\"グー\", 1:\"チョキ\", 2:\"パー\"} # 出力をジャンケンの手に変換する辞書\n",
        "\n",
        "  for column in columns:\n",
        "    data_for_pred[column] = input(f\"{column}の値は？ → \")\n",
        "\n",
        "  pon = model.predict(pd.DataFrame(data_for_pred, index=['pred']))\n",
        "  print(f\"来週もまた見てくださいね〜！　ジャン・ケン・ポン！　「{trans_hands[pon[0]]}」　ウフフフフフ❤️\")\n"
      ],
      "metadata": {
        "id": "4nL7RV56qRmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.head()"
      ],
      "metadata": {
        "id": "QzGTHvY0AlvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 勝負\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2']\n",
        "sazae_janken(loaded_model, columns)"
      ],
      "metadata": {
        "id": "qZEFjmb1AceA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 参考文献\n",
        "- サザエさんじゃんけん研究所 公式ウェブサイト、サザエさんじゃんけん研究所、http://park11.wakwak.com/~hkn/\n",
        "- サザエさんじゃんけん白書、サザエさんじゃんけん研究所、http://park11.wakwak.com/~hkn/report.pdf\n",
        "- Sazae_R、八寿、https://github.com/yaju/Sazae_R\n",
        "- https://w.atwiki.jp/sazaesannokiroku/pages/21.html\n"
      ],
      "metadata": {
        "id": "cwMC0wpkdk_h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# オマケ\n",
        "本colab製作者の試行錯誤履歴"
      ],
      "metadata": {
        "id": "AwmEaE8sqt8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 早速作っていこう\n",
        "df_all['hand_pre3'] = df_all['hand'].shift(3, fill_value=\"パー\")\n",
        "c = pd.get_dummies(df_all['hand_pre3'], drop_first=True).set_axis(['チョキ_pre3', 'パー_pre3'], axis=1)\n",
        "\n",
        "df_all = pd.concat([df_all, c], axis=1)\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "hDlYAboxWkBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2', 'チョキ_pre3', 'パー_pre3']\n",
        "janken_eval(lr_model, df_all, columns)"
      ],
      "metadata": {
        "id": "WA2HwAotXDLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all['is_first_pre'] = df_all['is_first'].shift(1, fill_value=0)\n",
        "df_all['is_event_pre'] = df_all['is_event'].shift(1, fill_value=0)\n",
        "\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "ZunqF1EkgJ1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'is_first_pre', 'is_event_pre', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2', 'チョキ_pre3', 'パー_pre3']\n",
        "janken_eval(lr_model, df_all, columns)"
      ],
      "metadata": {
        "id": "DGlv3Du0gv0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = pd.get_dummies(df_all['month'], drop_first=True).set_axis(['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12'], axis=1)\n",
        "\n",
        "df_all = pd.concat([df_all, f], axis=1)\n",
        "df_all.head()"
      ],
      "metadata": {
        "id": "bYGXXfUdtQAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model = LogisticRegression()\n",
        "\n",
        "columns = ['is_first', 'is_event', 'is_first_pre', 'is_event_pre', 'チョキ_pre1', 'パー_pre1', 'チョキ_pre2', 'パー_pre2', 'チョキ_pre3', 'パー_pre3',\n",
        "                '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12']\n",
        "janken_eval(lr_model, df_all, columns)"
      ],
      "metadata": {
        "id": "B8d0TvfotQDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 次回予告の役\n",
        "# 波平→フネ→マスオ→カツオ→ワカメ→タラちゃんの順\n",
        "nexters = {0:\"波平\", 1:\"フネ\", 2:\"マスオ\", 3:\"カツオ\", 4:\"ワカメ\", 5:\"タラちゃん\"}"
      ],
      "metadata": {
        "id": "y0V2mE5RqP_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# サンプルデータの生成\n",
        "np.random.seed(0)\n",
        "rounds = np.arange(1, 11)\n",
        "hands = np.random.choice(['guu', 'choki', 'pa'], size=10)\n",
        "df = pd.DataFrame({'round': rounds, 'hand': hands})\n",
        "\n",
        "# 各手のカウントを初期化\n",
        "counts = {'guu': 0, 'choki': 0, 'pa': 0}\n",
        "\n",
        "# 各手のカウントを更新する関数\n",
        "def update_counts(row):\n",
        "    global counts\n",
        "    counts = {k: counts[k]+1 for k in counts} # 1を足す\n",
        "    counts[row['hand']] = 0 # 出した手のカウントを初期化\n",
        "    row['guu'] = counts['guu']\n",
        "    row['choki'] = counts['choki']\n",
        "    row['pa'] = counts['pa']\n",
        "    # if counts[row['hand']] == 1:\n",
        "    #     counts = {k: counts[k]+1 for k in counts}\n",
        "    return row\n",
        "\n",
        "# applyメソッドで各行に対して更新を実行\n",
        "df[['guu', 'choki', 'pa']] = df.apply(update_counts, axis=1)[['guu', 'choki', 'pa']]\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "pwcgtmN9dVMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "\n",
        "# # 完成したい文字列\n",
        "# target_str = \"ドドスコスコスコ\"\n",
        "# rand_str = \"\"\n",
        "\n",
        "# # ランダムに文字列を出力しつづける\n",
        "# while True:\n",
        "#     rand_str_tmp = random.choice([\"ドド\", \"スコ\"])\n",
        "#     rand_str += rand_str_tmp\n",
        "#     print(rand_str, end=\"\")\n",
        "    \n",
        "#     # target_strと一致しているかどうかを確認する\n",
        "#     if len(target_str) <= len(rand_str):\n",
        "        \n",
        "#         # target_strが空になった場合、プログラムを終了する\n",
        "#         if rand_str.endswith(target_str):\n",
        "#             print(\"ラブ注入❤️\")\n",
        "#             break\n"
      ],
      "metadata": {
        "id": "sx3OLbTZl2ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LICENCE\n",
        "\n",
        "このcolabは[githubのREADME](https://github.com/agu-piedpiper/piedpiper-python)にて記載の通りのライセンスに従います。"
      ],
      "metadata": {
        "id": "MUa4tl1bbFK9"
      }
    }
  ]
}